{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Chrome History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_history = pd.read_json('/bigtemp/ahs5ce/CS6501_Projects/Project1/data/raw/Takeout/Chrome/History.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_history = chrome_history.T\n",
    "chrome_history = chrome_history['Browser History']\n",
    "chrome_history = pd.DataFrame(chrome_history.tolist())\n",
    "chrome_history['time_usec'] = pd.to_datetime(chrome_history['time_usec'], unit='us')\n",
    "chrome_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_history.to_csv('/bigtemp/ahs5ce/CS6501_Projects/Project1/data/processed/chrome_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess YouTube History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-5.3.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
      "Downloading lxml-5.3.1-cp310-cp310-manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-5.3.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      video_id                                    video_url  \\\n",
      "0  0R1LKPRwxR4  https://www.youtube.com/watch?v=0R1LKPRwxR4   \n",
      "1  tT4br0ZKjQ0  https://www.youtube.com/watch?v=tT4br0ZKjQ0   \n",
      "2  msol7hNyTkQ  https://www.youtube.com/watch?v=msol7hNyTkQ   \n",
      "3  f7Cz65ef8iM  https://www.youtube.com/watch?v=f7Cz65ef8iM   \n",
      "4  eKP0hOXg6Vo  https://www.youtube.com/watch?v=eKP0hOXg6Vo   \n",
      "\n",
      "            timestamp  \n",
      "0 2025-02-17 10:30:30  \n",
      "1                 NaT  \n",
      "2 2025-02-17 10:07:49  \n",
      "3 2025-02-17 10:07:10  \n",
      "4 2025-02-17 10:05:56  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Compile regex outside the loop for efficiency\n",
    "video_regex = re.compile(r'watch\\?v=([^&]+)')\n",
    "\n",
    "# Read the entire HTML file (if it's very large, consider reading in chunks or filtering beforehand)\n",
    "with open('/bigtemp/ahs5ce/CS6501_Projects/Project1/data/raw/Takeout/YouTube and YouTube Music/history/watch-history.html', 'r', encoding='utf-8') as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "# Parse with lxml for improved performance\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "entries = []\n",
    "\n",
    "# Use a CSS selector to directly target watch history entries\n",
    "# Each entry is in a <div> with these specific classes\n",
    "outer_cells = soup.select('div.outer-cell.mdl-cell.mdl-cell--12-col.mdl-shadow--2dp')\n",
    "\n",
    "for cell in outer_cells:\n",
    "    # Narrow down to the content div that holds the video details\n",
    "    content_div = cell.select_one('div.content-cell.mdl-cell.mdl-cell--6-col.mdl-typography--body-1')\n",
    "    if not content_div:\n",
    "        continue\n",
    "\n",
    "    # Use a CSS selector to find the first <a> tag with a YouTube video link\n",
    "    video_link = content_div.select_one('a[href*=\"watch?v=\"]')\n",
    "    if video_link:\n",
    "        video_url = video_link.get(\"href\")\n",
    "        video_id_match = video_regex.search(video_url)\n",
    "        video_id = video_id_match.group(1) if video_id_match else None\n",
    "    else:\n",
    "        video_id, video_url = None, None\n",
    "\n",
    "    # The content_div text (split by <br> tags) should contain:\n",
    "    #  - \"Watched\"\n",
    "    #  - Video title (as the text of the <a> tag)\n",
    "    #  - Channel name\n",
    "    #  - Timestamp (last element)\n",
    "    text_parts = list(content_div.stripped_strings)\n",
    "    timestamp = None\n",
    "    if len(text_parts) >= 4:\n",
    "        # Assume the last part is the timestamp string\n",
    "        timestamp_str = text_parts[-1]\n",
    "        # Replace any non-standard spaces (like narrow no-break spaces) with regular spaces\n",
    "        timestamp_str = timestamp_str.replace('\\u202f', ' ')\n",
    "        # Remove the timezone (last token) if present, to ease parsing\n",
    "        tokens = timestamp_str.split()\n",
    "        if len(tokens) > 5:\n",
    "            timestamp_str_mod = \" \".join(tokens[:-1])\n",
    "        else:\n",
    "            timestamp_str_mod = timestamp_str\n",
    "        try:\n",
    "            # Adjust the format if your timestamps differ\n",
    "            timestamp = datetime.strptime(timestamp_str_mod, \"%b %d, %Y, %I:%M:%S %p\")\n",
    "        except Exception as e:\n",
    "            # If parsing fails, fallback to storing the raw string\n",
    "            timestamp = timestamp_str\n",
    "\n",
    "    entries.append({\n",
    "        \"video_id\": video_id,\n",
    "        \"video_url\": video_url,\n",
    "        \"timestamp\": timestamp\n",
    "    })\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(entries)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NaT in timestamp: 3934\n"
     ]
    }
   ],
   "source": [
    "missing_count = df['timestamp'].isna().sum()\n",
    "print(\"Rows with NaT in timestamp:\", missing_count)\n",
    "df.dropna(subset=['timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/bigtemp/ahs5ce/CS6501_Projects/Project1/data/processed/youtube_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read the HTML file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/bigtemp/ahs5ce/CS6501_Projects/Project1/data/raw/Takeout/YouTube and YouTube Music/history/watch-history.html\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 8\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m entries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Each watch entry is contained in a div with the class \"outer-cell mdl-cell mdl-cell--12-col mdl-shadow--2dp\"\u001b[39;00m\n",
      "File \u001b[0;32m/bigtemp/ahs5ce/.venv/lib/python3.10/site-packages/bs4/__init__.py:473\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/bigtemp/ahs5ce/.venv/lib/python3.10/site-packages/bs4/__init__.py:658\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarkup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[0;32m/bigtemp/ahs5ce/.venv/lib/python3.10/site-packages/bs4/builder/_htmlparser.py:467\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    464\u001b[0m parser \u001b[38;5;241m=\u001b[39m BeautifulSoupHTMLParser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     parser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/html/parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/html/parser.py:172\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    170\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_starttag(i)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[0;32m--> 172\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_endtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<!--\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[1;32m    174\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_comment(i)\n",
      "File \u001b[0;32m/usr/lib/python3.10/html/parser.py:420\u001b[0m, in \u001b[0;36mHTMLParser.parse_endtag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_data(rawdata[i:gtpos])\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m gtpos\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_endtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_cdata_mode()\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gtpos\n",
      "File \u001b[0;32m/bigtemp/ahs5ce/.venv/lib/python3.10/site-packages/bs4/builder/_htmlparser.py:213\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_endtag\u001b[0;34m(self, name, check_already_closed)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Handle a closing tag, e.g. '</tag>'\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m:param name: A tag name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m   e.g. '<tag></tag>'.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# print(\"END\", name)\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_already_closed \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malready_closed_empty_element:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# This is a redundant end tag for an empty-element tag.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# We've already called handle_endtag() for it, so just\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# check it off the list.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# print(\"ALREADY CLOSED\", name)\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malready_closed_empty_element\u001b[38;5;241m.\u001b[39mremove(name)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Read the HTML file\n",
    "# with open('/bigtemp/ahs5ce/CS6501_Projects/Project1/data/raw/Takeout/YouTube and YouTube Music/history/watch-history.html', 'r', encoding='utf-8') as f:\n",
    "#     soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "# entries = []\n",
    "\n",
    "# # Each watch entry is contained in a div with the class \"outer-cell mdl-cell mdl-cell--12-col mdl-shadow--2dp\"\n",
    "# for cell in soup.find_all(\"div\", class_=\"outer-cell mdl-cell mdl-cell--12-col mdl-shadow--2dp\"):\n",
    "#     # Find the content div that holds the details (video link, channel, and timestamp)\n",
    "#     content_div = cell.find(\"div\", class_=\"content-cell mdl-cell mdl-cell--6-col mdl-typography--body-1\")\n",
    "#     if content_div:\n",
    "#         # Find the first anchor tag with a YouTube video link\n",
    "#         video_a = content_div.find(\"a\", href=re.compile(r'watch\\?v='))\n",
    "#         if video_a:\n",
    "#             video_url = video_a.get(\"href\")\n",
    "#             # Extract the video id from the URL\n",
    "#             video_id_match = re.search(r'v=([^&]+)', video_url)\n",
    "#             video_id = video_id_match.group(1) if video_id_match else None\n",
    "#         else:\n",
    "#             video_id = None\n",
    "#             video_url = None\n",
    "\n",
    "#         # The text inside content_div is structured with <br> tags.\n",
    "#         # Typically, the strings (in order) are:\n",
    "#         #   1. \"Watched\"\n",
    "#         #   2. Video title (from the video link)\n",
    "#         #   3. Channel name (from the channel link)\n",
    "#         #   4. Timestamp (e.g., \"Feb 11, 2025, 8:44:33 PM EST\")\n",
    "#         text_parts = list(content_div.stripped_strings)\n",
    "#         timestamp = None\n",
    "#         if len(text_parts) >= 4:\n",
    "#             timestamp_str = text_parts[-1]\n",
    "#             # Replace any non-standard space (e.g. narrow no-break space) with a normal space\n",
    "#             timestamp_str = timestamp_str.replace('\\u202f', ' ')\n",
    "#             # The timestamp string typically ends with a timezone (like \"EST\").\n",
    "#             # To simplify parsing, drop the timezone.\n",
    "#             # For example: \"Feb 11, 2025, 8:44:33 PM EST\" -> \"Feb 11, 2025, 8:44:33 PM\"\n",
    "#             parts = timestamp_str.split()\n",
    "#             if len(parts) >= 6:\n",
    "#                 timestamp_str_mod = \" \".join(parts[:-1])\n",
    "#             else:\n",
    "#                 timestamp_str_mod = timestamp_str\n",
    "\n",
    "#             try:\n",
    "#                 # Adjust the format string if your timestamps differ\n",
    "#                 timestamp = datetime.strptime(timestamp_str_mod, \"%b %d, %Y, %I:%M:%S %p\")\n",
    "#             except Exception as e:\n",
    "#                 # If parsing fails, you can store the original string or handle the error\n",
    "#                 timestamp = timestamp_str\n",
    "\n",
    "#         entries.append({\n",
    "#             \"video_id\": video_id,\n",
    "#             \"video_url\": video_url,\n",
    "#             \"timestamp\": timestamp\n",
    "#         })\n",
    "\n",
    "# # Create a DataFrame from the list of dictionaries\n",
    "# df = pd.DataFrame(entries)\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
